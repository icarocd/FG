share network contexts applied, these feature other, 0.057629095613083554
dimensions word created, because may vectors set, 0.05494537196822393
probabilities algorithms bayesian not numerical, form spark, 0.05696683209570199
embeddings words with like, embedding produces in located, 0.05311297863810044
so, data semantic takes turns purpose, 0.05828185439132452
common assigned the, implements create algorithms java, 0.058982022075642015
close embedding form, algorithm, 0.06234903785476655
tomas networks several, individual, 0.05200531428996766
which, related media large located works, 0.05643363168817609
its large on been created, not create discerned likevec can, 0.05559330181623697
of, wordvec advantages, 0.060337384048365056
hundred to positioned its share, been patterns similar, 0.053816595175177694
followervec vector extend set trained, why followervec, 0.04547744563145638
below together network several, be produces, 0.0562274898430761
such neural patterns, purpose possible form words, 0.05464252632757488
are net representations, without usefulness, 0.057463445478551436
corresponding embedding many hundred, because corpus typically researchers, 0.05527340115693212
these models, wild graphs close hundred, 0.05592992784399726
proximity data unique nets extend, we subsequently neural at, 0.05725858582001925
or states located each features, together, 0.05492255533272028
you, series in, 0.05463194624696671
and, to related deep not, 0.05998324829472416
these transitional wild processes, used version wild produces, 0.04345655788586102
tutorial has similar was, human distributed at, 0.05560037150654785
or advantages turns, spark neural, 0.059213019620312606
hundred common graphs, mikolov scala located, 0.05219994466058925
social, similar tomas bayesian vectors, 0.05247907903524396
of just well genevec has, series positioned takes networks genevec, 0.04364941273828894
algorithms space one processes, followervec input, 0.05325214177553356
are be embeddings, group well media produce nets, 0.055421113379052055
common, semantic while, 0.05547087604790927
those explained, with just, 0.05507643788630143
compared led, intervention mentioned how you proposed, 0.05363891327655651
at, create, 0.060006510466337204
are, trained, 0.06517711281776428
series parsing led, by all we net, 0.056192080342280556
researchers human dimensions, scala, 0.0587051795225762
vector, series turns version net, 0.05787439116132446
into, which embedding, 0.06390436816113443
word, each, 0.05952246114611626
because, how in, 0.06075849430141598
net typically can, symbolic team why each, 0.056177478812503484
or, embeddings assigned semantic, 0.0571845121311026
many its, understand network java google series, 0.05659183926570863
without, genevec similar high-dimensional, 0.06320968019280722
gpus features, dimensions, 0.06312437375702709
understand share, has, 0.05479836729655043
between tutorial common, wild words, 0.057331179189978966
well was mikolov, similarities neural simply, 0.05394887670228705
models, positioned words context proximity, 0.058101412836509755
text algorithms mathematically probabilities, mikolov produces can algorithm share, 0.05074318260401115
media another compared deep algorithm, similar compared several are, 0.042867951729941366
hundred, proximity trained form, 0.06199614656117367
output graphs algorithm genes, share vectors network why, 0.05615849397054501
created corresponding likes embeddings, algorithm java mind applied earlier, 0.053600203561300415
located usefulness co-occurring data, you detects in algorithms, 0.05361198340924457
on being, can produces, 0.06219006526309625
spark its java compared, its transitional corresponding bayesian individual, 0.045160891921035756
in form not all, co-occur, 0.06282651321934908
likelihood, series analysis implements, 0.057319947673459805
high-dimensional, several, 0.05848383903503418
proximity mind analysed, they corpus located deeplearningj create, 0.051008577091150586
those, located two-layer, 0.055215199231579894
all was sentences java, into, 0.058368842120245096
verbal co-occur, transitional located, 0.058142135230805726
models bayesian the discerned, features unique can, 0.05981810437724289
without all, produce while algorithms intervention, 0.0586025819272399
beyond deep can, which at, 0.06203506197634743
intervention or not these, extend researchers for possible, 0.05539020272108167
for well corresponding, may with, 0.05335400626842226
likevec another each vectors its, graphs related, 0.0544987373034127
with, in unique transitional, 0.05804625946347151
version, reconstruct all another create will, 0.06059293559929133
contexts share or, networks gpus intervention group, 0.055717023897572875
on has created in states, graphs processes advantages vectorspace looking, 0.055306744803823533
vectors by context looking, wordvec as which likes, 0.0551977995022405
vector neural its spark, the which, 0.06297394380068778
word, mikolov detects social graphs or, 0.05369055111864209
while, so input, 0.05675880425167456
scala bayesian, purpose, 0.06339472233082354
states, numerical, 0.054571572691202164
word, into context discerned looking common, 0.05474781902164817
being, playlists, 0.054993506520986564
embeddings gpus advantages, analysis, 0.058353898650179085
representations earlier parsing mind why, followervec, 0.05592774913932383
earlier space word, applied unique you located, 0.052215325427374004
reconstruct by symbolic in, related, 0.06175668148237466
nets models deep text tomas, on, 0.06042444071193487
series scala detects researchers, without works help, 0.055553248731145886
states transitional, been reconstruct likelihood, 0.056151227572353794
all without, be, 0.06024468296522274
net corpus mind, space can for, 0.052913238952176166
help, because probabilities be, 0.06009883390766003
with, reconstruct not media because, 0.060055436006983744
create representations, detects social using, 0.055396932958766576
other algorithms corpus features, we how, 0.05365735988783837
have why word tomas, deeplearningj context, 0.06064013018709421
intervention likes verbal group, compared they why java which, 0.055781217479259515
corresponding vector symbolic may in, its genes mathematically, 0.05431513509061533
likes or why vector, proximity be followervec, 0.053186959857986396
representations located, creates any positioned or was, 0.05275639341178835
embeddings models it above, to symbolic, 0.0496343166154325
being with two-layer vectorspace, well similar related these, 0.05342007343482971
media be networks word, to, 0.05880772071557399
parsing social without or dimensions, co-occurring several beyond to, 0.05379079521393478
corresponding simply, two-layer on how text, 0.05410633796961606
typically has it corpus, a why well they, 0.05367644418257475
likevec proposed likes purpose, can reconstruct symbolic linguistic, 0.057103350902795794
into high-dimensional without may, intervention probabilities corresponding is, 0.05676479906869121
how explained, genes looking will, 0.05754579367802704
possible dimensions code into, detects series reconstruct playlists, 0.055368929335512226
below, on playlists being probabilities, 0.0636355377198644
the probabilities, compared, 0.0649990022265315
have network are we algorithms, can semantic being many such, 0.055717114944410326
create applications human spark into, deep, 0.05388773227180243
similar deeplearningj common, mathematically detects sentences below, 0.05318384775878777
creates typically produce playlists net, each researchers the, 0.06165876167163228
are, we semantic, 0.06622163517394289
understand all genevec linguistic been, space not analysed, 0.0544113124237352
any turns, space, 0.05629030185043998
network, tomas that led likelihood, 0.06346386585115268
human the create, representations genes, 0.06701614123240592
be gpus vector its simply, on similarities algorithm, 0.05631247843244697
researchers co-occurring, sentences, 0.06166944037380069
team, other help input wild trained, 0.057009698867471516
states corresponding, deep gpus without, 0.057116218848536855
creates hundred another mathematically these, probabilities as mind embeddings unique, 0.05378735446754992
subsequently unique they turns, that above several, 0.058269518702131554
embedding does below, its close, 0.0590386068952056
to words space are, extend any, 0.053769901167327536
tutorial can produce, works advantages for similar, 0.05485526532511532
positioned, is purpose, 0.0581561818918623
symbolic how, they and located vectorspace can, 0.05533627355983033
intervention version large, subsequently shallow similarities, 0.05572383544323295
words located will dimensions, they mentioned, 0.05579455289317854
human large data purpose symbolic, genevec feature similarities human, 0.045610950314728914
typically likevec likelihood, numerical typically, 0.038461484148337007
likevec, typically processes discerned, 0.05572825715860445
may wild team symbolic, together compared data, 0.05745131034135211
scala bayesian, all we looking mind several, 0.05425550387055204
as turns word algorithm, as semantic one mathematically, 0.04244802809118107
assigned it mentioned why, words located, 0.05786348841306567
its below, are several, 0.06498849426180124
positioned mathematically into, help can has corpus typically, 0.0534127210998337
team for typically, proximity tutorial you, 0.050575182093733385
similarities a, playlists, 0.06271965589061379
semantic, sentences understand detects output, 0.055708078586453574
at, above positioned reconstruct vectorspace below, 0.06236974484211802
algorithms detects all researchers a, wild extend, 0.055980382428132
as is analysed, mind, 0.05694725585547879
well, code co-occurring corresponding produce space, 0.05528819078776092
genes unique, human high-dimensional two-layer, 0.054843090388534944
looking net, why help analysed playlists vectorspace, 0.05424085962798074
produce, extend its models two-layer, 0.05669553303031996
they, which group how co-occurring, 0.05553544699680805
symbolic those tomas, purpose corpus those not create, 0.045005604387440334
being as, can other compared, 0.057974866607322845
hundred numerical we, media one related patterns data, 0.057386394460016206
are, discerned as usefulness turns, 0.06187245537156612
parsing analysis can mind, sentences beyond looking, 0.05385521663354408
that wordvec scala, mathematically at, 0.06177097381484151
trained models, well using, 0.05216812321269512
is states tutorial mind, related help linguistic being takes, 0.054621068885372576
transitional been google another, common data, 0.054802191607061776
created extend embedding shallow, corpus linguistic, 0.05557065181418508
form embedding to tutorial, algorithm trained on not, 0.05717412785203196
any beyond while sentences output, created, 0.05949481740747392
just extend unique creates, hundred used compared states embeddings, 0.053833009738036244
verbal does assigned, wordvec states set shallow team, 0.059685813100769675
works, version led is deeplearningj semantic, 0.05716342825406045
social numerical be, followervec you mind tutorial genevec, 0.054223165875590486
each, you transitional researchers group, 0.05637000186480954
latent states corresponding followervec, not algorithm simply applications, 0.052430083878619596
two-layer may, purpose together team scala the, 0.061571730501847714
into, gpus code by, 0.05955280328975658
be we, on space, 0.056810917997555804
has and, earlier below two-layer, 0.05642337907755689
how gpus the, purpose below code, 0.061411633214105234
it mentioned semantic data has, distributed using led, 0.05389645857887126
well distributed reconstruct, its well latent network, 0.04218753104725747
input scala of, media, 0.058541883998054864
large in usefulness be reconstruct, simply algorithm text common, 0.053084930446350945
be just google above, genevec as used, 0.05699647507453656
analysis they, used neural simply followervec, 0.05610144303172454
was between code neural common, embedding genes, 0.05656627014952749
followervec, these for, 0.059845889609724284
implements simply embeddings, while, 0.058362013753364084
verbal another individual shallow group, the these, 0.06233894209465981
simply, while, 0.057018231600522995
input embeddings several we, symbolic input followervec possible on, 0.04528896948166564
transitional and several, mikolov shallow of that usefulness, 0.057756103330286135
close symbolic semantic, many in because, 0.056844867736573323
mentioned, a, 0.0670965313911438
many group, contexts, 0.0633491760708578
so word wordvec explained, probabilities applied wild, 0.056002312197236614
mind sentences individual google shallow, proximity, 0.05495600497885868
sentences followervec other, reconstruct positioned, 0.05570693764808415
code how, led, 0.055956387150026855
latent creates, mind detects shallow social scala, 0.053270316092204305
located two-layer, to one network, 0.05539163491819329
algorithms related mind produces, explained nets how just embeddings, 0.05492297987961769
likevec analysed such, one, 0.05815884048733682
discrete, in of, 0.07140601859170943
discrete intervention below contexts positioned, like corresponding we its, 0.05489468364220708
simply vector its, similar simply version gpus, 0.0411425684462282
mind followervec proximity, word applied networks its, 0.05466719143967846
together embeddings so, a researchers it, 0.05352474139036462
genevec being, how human, 0.051416617414444685
co-occurring deep likes text advantages, proximity how neural states used, 0.05274505391009152
algorithm, usefulness may as which, 0.05961202977418154
human group graphs code, intervention patterns, 0.05749867072982714
gpus possible, scala neural, 0.05407756421112642
likelihood mikolov verbal two-layer was, those produces tomas we, 0.05343891992360727
distributed which, looking explained together, 0.05745057581170526
beyond, two-layer patterns is located those, 0.059183405450588464
positioned creates java, intervention similar they beyond data, 0.05242423979346286
distributed analysed contexts produces mentioned, being while detects, 0.056214342292210354
we context, another parsing numerical has, 0.052882478617921475
wordvec scala, probabilities explained, 0.06334772204305976
each created help possible, many, 0.05499431606165134
below neural playlists genevec without, wild another just likevec patterns, 0.055450748909235
analysis or subsequently, likes led does analysed turns, 0.05661455100633566
create proposed context, many, 0.056386944473645506
help, researchers context discerned group, 0.05895768661922775
proposed was input, mikolov tomas was, 0.03771749478599291
space, set sentences, 0.059608007984474305
group into can, applied created for mentioned algorithms, 0.054599594164932694
net vectorspace, you patterns proposed, 0.05606678863390753
beyond such likes takes vector, spark between using to, 0.0541529998665832
beyond, discrete positioned share group any, 0.06123434969078302
scala words subsequently without possible, being corpus produce mikolov all, 0.05628666491103172
being, possible models applied subsequently or, 0.05672084002157301
vectorspace feature, creates set analysis, 0.058297076262812476
version high-dimensional creates between those, you positioned created, 0.05356468327847055
data for does, together feature, 0.05593004142962044
you help wordvec led its, you just or numerical, 0.0447050370596543
like looking usefulness, transitional, 0.050426709534114954
share feature word located, help which, 0.058715719612501556
with series, applied nets advantages input, 0.05707316455312818
how context set below, which just deep, 0.057478154151545696
representations net, genevec vector trained with, 0.05517897454865836
be intervention dimensions, vectorspace, 0.05681555636148267
sentences series compared, parsing applied space because, 0.056682308734975054
they may that neural one, version, 0.05700400139416978
not high-dimensional, output like distributed, 0.05307452833981671
any implements or, its have likelihood will, 0.05768707049016258
media vectors text bayesian, any well, 0.054129669905640186
words, large, 0.06504136323928833
linguistic, graphs text analysed using, 0.054913435564530086
unique any does numerical, not, 0.05811539971902966
created output applied, well neural related, 0.05603332831962253
processes without works version, vector linguistic followervec simply, 0.05578581246122718
scala graphs into with, on, 0.05977267300995067
net it likevec into corresponding, features, 0.057453392204730204
typically are transitional, can, 0.06333658454464641
was or network embedding together, similarities corpus states those, 0.05570682477531731
models output so be, with nets understand, 0.05420925320354469
high-dimensional data unique another latent, been form group they, 0.05583309788615629
create you, co-occur, 0.06140004502849654
below by group similarities, extend we, 0.0601153542681802
possible help assigned co-occur, because, 0.05925132892159559
compared was just another two-layer, subsequently latent deeplearningj are, 0.0534955543521069
word mentioned two-layer, not probabilities unique these of, 0.05678603747811048
looking produce they as, dimensions, 0.06067280888889544
will turns, corresponding context vectorspace, 0.05788437533453945
human, we social common likes analysis, 0.05711671288785339
was applied so applications such, text models, 0.05050443754629269
researchers probabilities another by, bayesian individual, 0.057388983635619284
wordvec corresponding, social analysed latent you several, 0.058309404781277475
looking networks nets set, similarities java, 0.056183240840332584
close using wordvec, one it together bayesian just, 0.056960675385663705
human, while so researchers created with, 0.062255151899570225
between, create mikolov, 0.0564708113225922
hundred version, been and, 0.058894389045264575
takes below, implements as, 0.06173208616523817
other set genevec states, turns data each subsequently explained, 0.056617477530092
co-occurring likes related, how one likes produces co-occurring, 0.03212954331011632
version code, word earlier vectors, 0.055404871349777854
close researchers be playlists mathematically, parsing co-occurring, 0.05739585228840411
nets together latent gpus, scala form intervention corresponding, 0.053022130963727834
unique and is, likes we, 0.05865141769153869
at mentioned several co-occurring graphs, without semantic version, 0.057422513351061855
possible has researchers led like, series, 0.05687271404506267
form analysis between, on data, 0.0538109027765033
team embeddings produces created, below why been genevec on, 0.05829809739660993
typically likelihood, space net earlier input on, 0.05664044133327007
may because, together between, 0.054151870098963384
form so at, neural tutorial, 0.06055174990594153
explained mind, mentioned mathematically words feature, 0.05541625649314374
deep possible creates may positioned, created deeplearningj verbal team understand, 0.05383865164029598
bayesian latent, set, 0.06129218216239661
analysed, for and vector, 0.05460032109149019
well the vectorspace without applications, mikolov, 0.06436787482683956
how not proximity, is or output well, 0.05110120754185674
applications such the java vectorspace, for, 0.05947420607452541
not a being embedding, takes code tomas individual, 0.05250156458734349
on similarities like beyond extend, not share, 0.05685844302354456
explained you net into analysis, the genes code, 0.0573375448652134
because can tomas, so data co-occur explained, 0.054762278172846963
verbal team embedding they, will playlists representations data without, 0.05354697846214325
well it any, high-dimensional, 0.0565035748425323
algorithm co-occurring, by vectors deeplearningj hundred, 0.0563851795983687
linguistic playlists features many, neural, 0.05583814991269819
net, into, 0.06043568253517151
gpus spark graphs typically, group create it, 0.053984034642284705
how analysis feature several, typically verbal each group, 0.05566258207067288
genevec have how verbal so, have typically probabilities in, 0.04587565250291676
networks, purpose at compared used extend, 0.06139064433985948
spark related researchers without which, usefulness, 0.061303847640319174
contexts purpose possible in, will, 0.05829520303183794
how social corresponding, gpus above similarities, 0.05164890301279712
produces high-dimensional spark playlists, vectors, 0.05589253097578883
purpose works team in, proximity, 0.05331031324246899
purpose be graphs are, another latent, 0.061441608467146754
each google compared code advantages, positioned, 0.056587124591623246
beyond corpus usefulness, embeddings, 0.05209320689374308
create is like, by extend compared applied, 0.055536284888491556
social into wild for representations, which together, 0.05626604396016896
wild, with parsing dimensions, 0.05697149996586815
wild latent probabilities, with embeddings corpus embedding, 0.053875112021110304
those purpose, co-occur similar parsing on extend, 0.05684656311249733
not, graphs java be its likes, 0.05555090309760123
models dimensions or mikolov, on verbal network, 0.057630682926710454
feature, was into led well, 0.05995319130635262
mind or typically, graphs mikolov likelihood, 0.05730446909301725
produce representations led probabilities distributed, discrete, 0.06070624047338367
to each which, which common verbal code, 0.04016853742230968
google not genes words dimensions, beyond, 0.05624980924475193
so, corpus understand, 0.05505860408965871
algorithms input used code those, we so between vectorspace, 0.05218765900109708
intervention, distributed transitional mind, 0.05796572772513287
set, in tutorial one corpus likes, 0.0612244724216342
feature assigned, probabilities, 0.06263706947554648
being common created, which networks analysed, 0.05597073208266491
not similar, text spark, 0.05438595406796784
one proposed feature purpose, create contexts, 0.05645320923676342
text many patterns a, each words co-occur, 0.0574284393236757
linguistic assigned discerned corpus, form intervention simply embedding corresponding, 0.05268413777630031
trained by contexts set vectors, discerned does, 0.05665573803727627
output, will created, 0.05846537269245833
text space group trained, nets, 0.05744749619025737
a subsequently two-layer understand, these common wordvec, 0.05749109815404834
proposed other, typically spark be genes high-dimensional, 0.05252283636395931
mentioned each, applied possible detects creates, 0.05367922863034531
likevec, used extend gpus, 0.058204413577618984
being, understand, 0.061155449599027634
takes, they group trained, 0.05973465613645429
similarities beyond to parsing symbolic, produce common looking hundred with, 0.051830742266705636
feature latent is mentioned genes, help created produce usefulness, 0.052056285134205224
words understand two-layer nets genes, other, 0.05683556061014533
located will one, to has sentences, 0.05223836124038941
neural between are using not, for while of by all, 0.0556828622341156
feature any, corpus, 0.059341265318423514
well, such wild being applied similarities, 0.05656733507830128
wordvec output that not, analysis corpus media social, 0.05553025166276749
we google likes all, discrete processes, 0.06024606592676416
any been, embeddings, 0.05203776186616905
which two-layer, patterns, 0.05759754178889282
likevec processes team patterns like, models does, 0.05860191870386228
been code neural compared, usefulness together applications, 0.051639676183265856
which used, produces sentences below applied have, 0.057307537776743626
extend positioned version the networks, with are, 0.05684927131964341
it produces like genes tutorial, just reconstruct several, 0.05542440303740929
while have proposed with all, will states so that, 0.05500335277441442
team, vectors applications, 0.05958526044758782
processes wordvec team mind, for, 0.05651876372721792
similar proximity intervention, media algorithms, 0.055169625473413385
created, explained subsequently, 0.06506433458597585
semantic human, positioned they above playlists these, 0.05556220655844509
net, algorithms models discerned, 0.05670279186747839
advantages that code all, transitional feature context a, 0.056020975949708375
are, similarities, 0.06471997499465942
contexts neural, all spark, 0.06275544941382483
as playlists, implements another, 0.06030255588065088
media detects, created, 0.06028176587983966
input dimensions playlists mentioned, into using, 0.055307107505142686
tomas on researchers space, algorithms help discrete we, 0.05732351564683951
the mathematically have latent of, understand co-occurring, 0.05622495431281776
each co-occurring, individual, 0.058200520114064215
hundred, semantic verbal dimensions does any, 0.05536521542567387
proximity in, vectors distributed these, 0.054905924025010606
individual positioned networks, code neural word mind, 0.0545318149765868
on common verbal, set, 0.056251521955922076
representations close assigned co-occurring, scala algorithm, 0.0543959054005146
analysed be between, net representations will one two-layer, 0.0561188944701215
network have, with has, 0.05792081270366907
not possible earlier, feature share typically related why, 0.05763259835988352
context another scala neural such, spark, 0.059897392329195144
applied deeplearningj that create just, in tomas vectors and, 0.05926484285329953
context bayesian its led playlists, with detects, 0.055973758589333296
team you genevec, dimensions hundred, 0.05909101155955852
how large just, analysed, 0.057088269362508215
networks or without usefulness group, discrete so mikolov discerned, 0.05550970652689934
which one linguistic, playlists, 0.0644852223643311
led distributed vectorspace created co-occur, mind positioned, 0.05332628726279735
genes many input shallow, similar vectors subsequently, 0.055491389434637335
typically been such, team followervec other states, 0.055475403227781474
that version words, one, 0.0611204875418953
common has be probabilities, produces networks representations scala, 0.05560887653494626
common tutorial compared purpose, into, 0.05726039959525131
nets it algorithms, looking, 0.055580835806988464
all, applications text does subsequently, 0.05577240716558322
will analysed other context, simply symbolic extend, 0.05265234243332216
they unique co-occur java, genevec set produce, 0.056685997082043436
tutorial understand mikolov, because just, 0.05575983999058733
analysed co-occurring, is social genevec wordvec, 0.05449164345364273
and set, wordvec network analysed by they, 0.05726226428655982
have, embedding you symbolic semantic with, 0.06231635091146082
the between, by deeplearningj, 0.06054978614225983
turns tutorial, produces, 0.06489964416676759
feature detects researchers deep hundred, why, 0.059751252053730185
you on assigned network, embeddings, 0.05177057406993025
embeddings space a, they and another wordvec, 0.05445535351019161
researchers team, dimensions may, 0.06391627514538914
space output, distributed, 0.0599145068768654
each semantic created may, as media or scala have, 0.05707021107026673
its high-dimensional discrete, help patterns that likevec words, 0.05897037433285919
applied, between takes each features, 0.06046619239272549
sentences, other above the discerned, 0.05989814176455885
researchers such common, trained genes networks representations vector, 0.05726134739105343
at have vector, social followervec verbal in such, 0.057879767793446585
individual, used has subsequently team, 0.05983157558819745
large networks, semantic may deeplearningj applied, 0.0588491747205127
is can all mind we, mind beyond, 0.043095902779439095
have social, are, 0.057748439686372874
proximity takes all help, will unique, 0.058549365310762075
high-dimensional each dimensions, group, 0.058511524565889865
intervention between, so, 0.05831697278540582
can was they deeplearningj, detects analysed reconstruct common are, 0.05604118662992343
output genevec, because mind while, 0.05445404995961255
text, corpus simply, 0.05549039849473537
between analysed, high-dimensional create, 0.055160762247383595
does dimensions, detects embeddings, 0.05568223155021667
help, numerical into extend scala which, 0.05578074999343306
java on deep, word, 0.055041268057507774
deep, wordvec, 0.0655188113451004
mathematically, discrete explained group parsing discerned, 0.05679988078347668
above not scala to, another, 0.05205882075302861
earlier common extend, input vectorspace all advantages, 0.05687828418351096
close, wordvec with social earlier not, 0.05953852703590841
likes data, proposed verbal graphs models, 0.050653231162438174
context, latent such or graphs corresponding, 0.05648611593363583
models as media, help, 0.05429532668212919
sentences looking discrete may, vectors features the context, 0.059384209286840635
another how between mind, group at individual google neural, 0.056846914752268045
tomas, processes embedding as shallow, 0.06430765869492293
positioned, such latent just mentioned beyond, 0.05626189316378385
one because, on together deep takes, 0.05828115781102144
mentioned embeddings linguistic while, group above or, 0.05721586725253962
probabilities detects, related co-occur possible playlists common, 0.05792974848398566
takes share works, at other produces earlier, 0.05889520068706962
semantic produce, group co-occur, 0.06451593502005562
representations, network be, 0.061012932701781396
as models, above input takes, 0.057741741899518875
is, graphs, 0.06646378338336945
team that for, proposed net wordvec, 0.05545688014409022
located embedding intervention mathematically, many, 0.0519490830704663
co-occur they and, common gpus led co-occurring, 0.054473689105690365
because usefulness, advantages any, 0.05564519119662792
group because, probabilities which, 0.06025067364787311
output nets, those tutorial, 0.056799310469247404
produce mind without, have, 0.06178178080866329
similarities other, contexts patterns into high-dimensional typically, 0.05388701847643107
at form numerical, embeddings, 0.059456977094587984
co-occurring similarities likelihood, beyond related earlier processes been, 0.05623514379524464
genevec, bayesian and with above, 0.06229273496481963
text social deep earlier mathematically, beyond graphs possible advantages, 0.0540725470951058
embeddings produce word, reconstruct proposed below states, 0.05450720081199013
google, google set, 0.029576448723673817
tutorial those, so is, 0.05811362814296037
located share, probabilities, 0.057153923354029655
graphs, linguistic are spark words, 0.06337550796841085
they that code, two-layer, 0.05791000978940938
possible each, wordvec reconstruct probabilities followervec likes, 0.05845637442368269
neural, code possible transitional, 0.05888699289002544
does usefulness, applied likes genevec, 0.05532354728691329
linguistic will contexts been implements, scala how simply vectors, 0.05438466460390091
while, below takes at likevec, 0.06361640302522853
those close, are bayesian representations wild, 0.056543003737229855
be, intervention java, 0.056197672085305676
of, discrete simply analysed, 0.06480364562022105
followervec output, semantic shallow contexts works co-occur, 0.0576088242821902
contexts bayesian several, data networks for can input, 0.05567743542035052
intervention large similar, each led usefulness sentences, 0.05282446665230054
corpus distributed graphs produce representations, likes, 0.0550784517584011
latent why transitional analysis, words beyond takes, 0.057421630614662454
data being two-layer vectors, between help, 0.05486939173547365
for network, google, 0.06188648406912759
below possible, features assigned does wild discrete, 0.05951469225495458
produce one states the deep, creates, 0.061021327013142404
tomas, such those a just turns, 0.05786926774392947
without semantic features on series, likelihood above, 0.057343954788833855
created compared we graphs help, understand numerical, 0.05746848424051478
a two-layer for data proposed, media are feature share, 0.053942705777168275
deep each close dimensions wild, deeplearningj without, 0.055281910303242504
positioned gpus contexts discerned, located just it for, 0.054626293879700825
google hundred understand, it we, 0.061035488265726814
like many, wordvec extend the in is, 0.05722124080821872
input words followervec does deep, corpus series context, 0.051366802385003726
how led algorithm co-occurring, these we data google corresponding, 0.053370246514722705
distributed is, proximity purpose why share not, 0.05645668120083659
parsing detects, discerned, 0.05208144446704164
java analysed gpus discrete, likes applications states, 0.056609191535630925
symbolic between intervention in, produces gpus, 0.05550597054257989
used distributed simply shallow, space used words can wordvec, 0.046035121703346074
for help, graphs form explained mentioned similarities, 0.05165799647846818
processes as located, its, 0.057108082946731586
usefulness corpus human vectorspace, individual series create, 0.055255209095904555
such, simply turns discerned symbolic, 0.056065512291334565
tutorial sentences, or analysed large playlists, 0.05680646375068836
transitional led context analysed, it proposed space, 0.056719945326872065
each parsing algorithms, shallow or deeplearningj, 0.056867724832586036
processes can each implements analysis, analysis below been created probabilities, 0.04585452144636958
google with led set, has implements a such, 0.055401662052009255
its tomas co-occur semantic, many and at, 0.05438916196832183
are representations neural at individual, applied analysed other understand, 0.05664632253214344
contexts, java context understand, 0.0626103896816026
has implements, and, 0.059708395156776534
deeplearningj distributed vector at, followervec usefulness co-occur, 0.056323601794618455
features other or set works, probabilities similar explained several hundred, 0.055488916785255075
representations are on context set, genes implements, 0.05845963539979308
net, possible, 0.05203256756067276
why for positioned, proximity google, 0.05548133507484517
possible gpus, version each used have applications, 0.056385715970210734
data or located similar, google such share, 0.05289990172505809
gpus google group, at models subsequently form, 0.05774230898549158
output it states features, are, 0.06505677139570937
linguistic each the words symbolic, beyond java symbolic, 0.04743298259517452
team, each likes such implements, 0.060696359646610916
spark, team similar advantages features so, 0.05948448457849771
discerned related, semantic deeplearningj, 0.05635054969396442
human gpus tomas followervec analysis, implements bayesian corresponding, 0.055215794523097626
context created, reconstruct positioned bayesian using gpus, 0.05708136340156645
which patterns does, positioned as, 0.05456144768882188
while been compared simply, while you mind nets being, 0.04110516742765829
wordvec, mathematically located genes well compared, 0.06160603660476208
beyond, it, 0.05368436127901077
wordvec has semantic wild together, it have form graphs set, 0.054271869360093775
team distributed text high-dimensional subsequently, linguistic probabilities, 0.05611567715114355
these assigned transitional similarities several, subsequently, 0.05715161154002398
takes distributed, may positioned wild, 0.05854620804118143
produce deep by so, looking mathematically human, 0.05850343158867469
series are reconstruct network beyond, java has, 0.05363201610408723
several, text set researchers takes it, 0.0609215673206985
can, patterns, 0.054017070680856705
a without how proposed genevec, mathematically, 0.05634624358087107
networks one simply graphs, into, 0.06199075843805261
explained contexts parsing without dimensions, models text used, 0.05609469991845414
common patterns feature are, together version, 0.05458736823219806
vector they produce related human, spark implements, 0.05487615117506832
discerned graphs why data models, how probabilities, 0.05662032576601543
with features beyond mathematically other, and have, 0.05504497058650851
other space turns high-dimensional features, one explained produces, 0.056890067309087904
algorithms turns corpus on genes, likes these, 0.05843998874806165
that vectorspace dimensions google because, likes tomas similar create, 0.053923069127929955
bayesian those likevec representations and, has, 0.05246906956166848
led many can, two-layer, 0.058833996270975436
co-occur beyond in, two-layer for earlier, 0.05676634180416135
applications turns analysis, contexts each, 0.05458699140388965
these, models patterns, 0.05768022870861366
models likevec, transitional it to, 0.05535618756613378
so, parsing series gpus corpus, 0.05459707668019459
just you, semantic advantages positioned net neural, 0.05604011220167651
followervec applications used, group, 0.062046647942512405
because algorithm deeplearningj, code bayesian, 0.052985644117437426
analysed, shallow they, 0.059239024471491576
genes analysis turns, individual co-occurring, 0.054502912497408955
at embeddings probabilities applications, features without is mikolov followervec, 0.05785428163212538
analysed patterns intervention, co-occur space gpus, 0.05360914554517783
analysis intervention, earlier, 0.06007369736921042
led with purpose, just can by, 0.056714240530182015
words contexts, it of large spark help, 0.06046532745478154
produce unique series mathematically, scala, 0.05740740186916105
was input, feature, 0.06104938535284996
embedding and human social close, tutorial close deeplearningj by, 0.04211988833156973
works may created, each probabilities so has, 0.056018489869484954
have mentioned while, followervec words possible, 0.05751645747946228
in, like positioned help similar turns, 0.06073052027533352
all applications, not any compared media, 0.05081374291318282
located why takes and subsequently, words can code, 0.055375134745297545
help on unique applications, proposed by, 0.058715608477592475
at, advantages, 0.0647055134177208
likevec bayesian, social looking purpose, 0.05478601484372175
created individual been works, close team, 0.05954608490069397
neural purpose, probabilities beyond for, 0.05972174648891193
group we net, analysis discrete individual advantages all, 0.05608455197465845
discerned because, it earlier spark linguistic been, 0.05484130225785971
network transitional, networks its for, 0.056291636170352266
proposed mentioned version, word researchers creates applications, 0.053705752948740935
as to so been, similarities embedding version algorithms two-layer, 0.054111235414224496
created have such wordvec semantic, playlists mathematically, 0.057776177325136215
close its, graphs embedding, 0.05925488346368074
another numerical purpose algorithms spark, so distributed, 0.05416344574953094
possible not, will or net may verbal, 0.054041170298007125
so google mentioned, bayesian genes, 0.05437797094782796
or embedding why, deep code implements, 0.05873439338438919
states explained, models neural created researchers, 0.057425333248209205
above distributed, is, 0.06420714703018963
together close proximity, high-dimensional algorithm, 0.05483077748141029
beyond why may processes, several mathematically been above, 0.05639891811478883
compared typically vectorspace, help, 0.05375996234233763
data intervention takes patterns while, numerical likes proposed, 0.054793520270829746
is, distributed looking to wild such, 0.05753675415409953
usefulness mentioned representations, produce such using states vectors, 0.056913243575972966
parsing shallow, like, 0.056626473380008716
all social version verbal those, java, 0.05713052038192749
mentioned explained, vectorspace gpus all media by, 0.056541831082761285
playlists earlier reconstruct, feature below, 0.05898416397479469
parsing assigned corpus embedding related, other network, 0.0570319403196685
text, between genes contexts tomas, 0.05520210225385893
numerical positioned, as, 0.05849885115197301
symbolic with extend, symbolic is, 0.03453324796681958
another produce as one, set of why, 0.057961827591405404
been mikolov, possible media analysed, 0.05798111273460902
assigned discerned on applications, mentioned neural transitional applied typically, 0.053766526531040665
output similar, have, 0.06370660287721082
contexts by, version, 0.06760899269593135
networks how co-occurring, looking verbal proximity trained as, 0.05395168583810463
input set team semantic, algorithm, 0.0542953767251987
it representations gpus, assigned many, 0.04960927903855285
by form set such, you media social neural, 0.055311244433820246
deep for these intervention on, with, 0.05647126220157072
creates vector using, below earlier, 0.05814664279353405
shallow, features any have, 0.058270339991600834
genevec located data share proximity, set we purpose have, 0.05443857217893936
word was distributed, one or bayesian output, 0.054495807133153544
several why proximity networks hundred, large such linguistic, 0.05408405426826479
can proposed representations input, analysis word two-layer these, 0.05416184039139189
subsequently two-layer, without intervention simply transitional, 0.05314846638947353
input above algorithm discrete with, spark, 0.061635359980370105
playlists, likelihood, 0.061721958220005035
extend, sentences, 0.05575023964047432
simply, related input its help the, 0.0584643571885407
into detects, that or help mind, 0.057532604487247764
embeddings be, all, 0.05279898507977464
set, subsequently share, 0.058194757313478736
be, applications, 0.05257541686296464
to representations, gpus does unique researchers mentioned, 0.059397219838332384
tomas positioned, turns reconstruct produce, 0.0602707533972734
corresponding can, models words algorithm, 0.05609348563858492
not, group close, 0.06744817262515425
embeddings to similarities, while output why feature can, 0.05756496325666781
been, individual, 0.0595899224281311
discrete, corresponding followervec are, 0.06423659199082564
been it, bayesian possible have creates numerical, 0.05463177613803894
group, vector, 0.06246234476566315
vectorspace such, to will, 0.058294015784123916
sentences series context can, to series human you, 0.04387849296679534
team likes, proposed these how be, 0.05305247113924474
assigned, context the, 0.07018480002247915
we, created many, 0.05948726775070466
networks transitional proposed, corpus, 0.05126823861117746
transitional series, semantic be without, 0.05943627205265509
patterns or below nets, compared symbolic it, 0.05479342134290481
be does input team, applied similar vectorspace, 0.052150680850748636
mathematically semantic usefulness, close analysed neural features, 0.05290296129542053
applied genes tutorial, tutorial just, 0.039209541187479865
with, shallow transitional to, 0.058761149555679106
form takes contexts have, below, 0.06308563416519761
common output produce applied those, compared located it, 0.05626680723392678
group similarities, mikolov parsing or analysis tomas, 0.05439904803839921
net unique wild it states, spark human are net, 0.046001562135931845
a trained produces, vector data extend, 0.05782463603929406
input gpus, because, 0.05438642449378967
between with output, to vector graphs, 0.055525430687517384
explained features symbolic tutorial, how other explained applied, 0.04016917367902398
trained well parsing, linguistic discerned typically, 0.053877919010737706
team without, because can these feature set, 0.06042870332203433
tutorial, implements, 0.06038173660635948
code for creates processes, context turns tutorial because, 0.05416313440071791
processes, analysed at, 0.06083064357489347
models turns, input features followervec applications, 0.050122888130708594
playlists social likelihood followervec many, space networks, 0.05261173696984351
you turns detects may net, into you representations, 0.043799094420893935
distributed subsequently dimensions, vector individual, 0.057058509572271446
does space, contexts why corpus reconstruct net, 0.055943466654125605
patterns was common tutorial, human spark been java, 0.05213616695528478
help high-dimensional below, tutorial distributed to proximity deep, 0.05269520111675133
high-dimensional two-layer, can, 0.057437282571000975
was they symbolic has neural, in dimensions, 0.05980596092524007
neural can corresponding create, have, 0.06309482294438779
other words deeplearningj such, by distributed, 0.056791424675345425
between, and, 0.0648510530591011
neural produce deep is, several, 0.05978203220460564
words advantages like, corpus genes likelihood researchers symbolic, 0.0539372755848602
implements discerned analysis does, so simply numerical verbal, 0.051627288118459284
distributed used mind graphs shallow, shallow deeplearningj, 0.04304201703861505
vectors embedding output share, distributed word in representations just, 0.05713839532389119
corresponding models embedding group analysis, being understand dimensions produces, 0.05708680749787464
because tomas a, the well, 0.06759182692906093
because, trained, 0.04933142289519309
tomas using beyond, two-layer corresponding group mikolov, 0.053892581148781676
scala using, corpus linguistic another by usefulness, 0.05861748860777617
java assigned, by because located, 0.06079488020491858
other proximity human models, understand patterns because help, 0.0544150759624131
was led researchers, using proposed, 0.05714282107575907
distributed gpus, linguistic numerical code discerned at, 0.05399676814839542
and media likelihood similar, mikolov related, 0.058433201480835674
looking has, common, 0.050907410119786854
works, large, 0.0627961978316307
shallow together they co-occurring networks, output at, 0.05500692962823659
produces to word all, in co-occur, 0.06023501898834854
mikolov parsing common dimensions such, corresponding embedding well, 0.05638011871015247
large, one earlier net used wild, 0.05802571395842134
embedding because tutorial why, creates assigned tomas, 0.053836964585123284
version form embeddings neural, mathematically on, 0.05938705610983074
purpose or of verbal, algorithm playlists, 0.0567637295464389
was, dimensions corpus wild semantic, 0.05738359574838728
text, similar does we, 0.052835933727621485
code they, probabilities corpus may create, 0.05633002525496483
intervention together shallow, scala have without, 0.05326571381285858
why contexts parsing into, output gpus processes well, 0.05750938457924686
space unique common, above deep large possible, 0.052770096918742626
have, analysis, 0.05628106743097305
bayesian create similar dimensions, individual, 0.059185364530887456
between applied analysis being, version followervec such, 0.05773462037637528
purpose, tutorial or, 0.06325390486419201
discerned it, compared any we, 0.05405511874927462
common version at proposed tomas, two-layer hundred, 0.054460798033493754
extend, transitional implements, 0.058329990948811174
feature several mikolov, unique data and likevec, 0.0569370561966754
detects to human discrete, large corpus, 0.053397808880992235
understand, not created, 0.05661610446871631
and a corresponding, simply why processes how, 0.05762098717505906
is co-occur processes may tutorial, series earlier explained likevec transitional, 0.05621610370733142
embeddings using space hundred similar, gpus, 0.054604436413574965
net space looking created, possible words, 0.05281937185376882
possible shallow may, these common word takes have, 0.05388535296407379
distributed dimensions, sentences and two-layer explained, 0.05742873000388965
graphs, discerned it another at as, 0.05983312822463512
below, algorithms representations usefulness embedding, 0.05943793121271953
code on processes be, discrete, 0.0630092981865406
network individual common, networks, 0.05913885842143631
words individual, of trained wordvec related by, 0.0556374780328691
or, embedding another patterns may, 0.059990281554132704
it corpus, other series has, 0.05342582243067904
as scala which corresponding analysis, algorithm genes, 0.05622303371082097
analysis, which hundred gpus wordvec, 0.05989217842531205
large, several in patterns shallow vectorspace, 0.05930179360628128
its processes to genevec, co-occurring typically mathematically high-dimensional located, 0.05513460421779603
shallow assigned processes, works, 0.05910873333260547
net graphs, feature or, 0.05614765735308453
nets processes, created each text, 0.05340058653760744
and above, creates produces tutorial vectorspace probabilities, 0.05581711500524282
with and you hundred, followervec, 0.054857925998905675
neural sentences we are, many led as two-layer its, 0.05528404508662969
may many, is mathematically, 0.05823701921488345
space team it as, does, 0.0597793688351959
hundred algorithm networks similarities has, its series because, 0.05531446105725042
transitional graphs, applications similar, 0.05079143411698937
applications spark, does, 0.0572549130370859
one context understand group discrete, numerical individual tutorial, 0.05658657066296587
several analysed while, its, 0.05920590216239826
usefulness bayesian using for, patterns linguistic possible, 0.05702267132291307
earlier social, co-occurring shallow, 0.055281219189707194
likelihood processes transitional, takes, 0.059197204216702284
series, likelihood, 0.05807574465870857
several that, contexts, 0.06596057164482771
or these, into unique media applications series, 0.05404955451357365
graphs does, related discrete algorithm genevec works, 0.05710228713873699
common probabilities not, reconstruct each states, 0.053425220314198295
intervention creates a semantic, common tomas analysed will, 0.05442777942717075
is for those, words vector been applied, 0.05327867090646678
feature version distributed co-occurring those, co-occurring subsequently, 0.043629474401865154
space not gpus proximity turns, create these followervec purpose output, 0.054852294954937694
on similar above led bayesian, is vectorspace form applied parsing, 0.05527482357792556
vectors numerical playlists probabilities can, representations, 0.05830145185902268
why, together, 0.0486888512969017
deep without which below discerned, these how, 0.05759876181380078
purpose, so implements you, 0.062054172538560784
and networks, team above version spark, 0.0593631810505595
vectors feature, have, 0.06532735033596307
you, well playlists set text help, 0.055956600809007884
implements close why google purpose, vectors human assigned embedding, 0.056774755529467015
corresponding several, version, 0.055843788838712494
embeddings high-dimensional similarities, code processes, 0.0535273461037172
contexts while latent verbal, co-occur any mikolov, 0.057408129240514166
like, word not proposed, 0.054928305338623244
been we words, of as, 0.0607667511107453
has advantages mind, just models genes input, 0.05408431886660985
spark, beyond may, 0.05320056876628845
models followervec as, data as parsing, 0.03664130325979571
net located trained, these reconstruct, 0.05888451586346898
likelihood, implements, 0.05693858116865158
numerical dimensions co-occur intervention, verbal scala probabilities, 0.05680202818992269
proposed while similar above, it for looking google discerned, 0.052761299908404426
which on proposed, proposed, 0.03878872235697215
scala has vectors at produces, space, 0.055334711725544924
models was, individual, 0.056148337692731996
are data wild, because looking detects positioned, 0.05819646627942412
proximity net similarities individual spark, just java, 0.056464787002819775
into space so likelihood will, code which similarities co-occurring was, 0.05235735760367811
below likes gpus, one sentences is vectorspace, 0.05789569346184523
output co-occurring team well, likes mathematically shallow one, 0.05338200401505455
parsing, reconstruct by produces mind, 0.05996370008236543
extend with, will sentences deep, 0.0532803309156745
version word typically discerned simply, likes scala was will analysed, 0.05214891632645726
all google can these, patterns share compared all, 0.04124611537160538
of mikolov, semantic advantages, 0.05837364373458922
human produces processes, feature may detects networks linguistic, 0.05706377717553431
social another, led, 0.053999059352628886
any its, reconstruct, 0.05890819172889739
vectorspace series team, neural purpose together or, 0.055981176420417306
mentioned of, discrete networks, 0.057838128454349935
words, scala verbal being beyond individual, 0.056459276090284435
genevec sentences each dimensions does, we, 0.056958806947866085
algorithm vector team symbolic vectorspace, not understand spark processes, 0.053359124859537185
or set, that may verbal can works, 0.057010942797744275
java followervec two-layer because mentioned, just any like, 0.05406767748707827
data, a implements large intervention has, 0.05843165481842757
between, such may looking be several, 0.05808343603981733
media we large together, neural produces, 0.0586533599307239
have understand social its, analysed, 0.05831675203477964
unique followervec why context, related using, 0.06103910109492391
mentioned they series, gpus of, 0.05891890774521505
co-occur set while media close, advantages wordvec group playlists, 0.05770898453514799
used processes like set, group features purpose implements deeplearningj, 0.05572483511496187
applied group and possible, trained, 0.054162218495612965
representations group genevec neural assigned, are help code, 0.05570029260424079
several, patterns followervec, 0.0607567930261232
together extend embeddings has output, states linguistic, 0.05460013008593619
beyond has, because similar, 0.049360778619814666
java feature, java embedding into, 0.03820502714950967
neural, on scala into using share, 0.06172027921459972
subsequently similar works each tutorial, how distributed semantic, 0.05335133223456473
team, likelihood graphs mentioned java, 0.0620229749764055
spark algorithms and, intervention vectorspace, 0.055405466782700426
because sentences algorithms dimensions, symbolic, 0.05525291442800313
produces they genevec genes simply, analysed advantages numerical all being, 0.05410478432371617
a and that, patterns words or works, 0.05450888456677713
creates two-layer into, may other, 0.05672148313233099
detects typically, the input, 0.05739676394949853
how models, understand compared will above, 0.05362738626996428
transitional advantages have, without scala above genes, 0.05705034457785018
each possible similarities, has tutorial corresponding between, 0.05233206033874473
of algorithm, proposed team while at, 0.05648623341589607
likelihood java mentioned does, looking, 0.05394686232237332
is, explained, 0.06116040423512459
being purpose java, created, 0.06430963592605402
models is, feature together analysis those used, 0.056354895343523476
algorithms, numerical nets, 0.053133822817554695
close understand being creates genes, can series positioned between, 0.052983077220867945
individual corresponding applications, patterns output, 0.05668706364698365
by on its tomas, all group mentioned by, 0.04423313812701404
context mind, at features, 0.055793304735399785
or contexts each why, set with trained, 0.058776738271064624
takes, semantic how, 0.05207281675085425
have, produces linguistic without, 0.06190462631040858
is wordvec word corresponding, at tomas version, 0.057270122277089275
individual common, one models are by, 0.059090895985513925
into, applied into just symbolic produce, 0.049619484033513066
all human applied applications, contexts, 0.06102375304089114
possible context all input, its analysed and being, 0.054360587998135014
intervention genes produce assigned likes, similarities deeplearningj used applied, 0.05457490749886968
at numerical applications like, been, 0.05812471418507025
works tutorial, numerical, 0.05630269236245751
models other below, is human earlier its, 0.05640263335149329
has human context share, led corresponding, 0.05674516390318051
while, followervec takes individual dimensions, 0.06141554851095006
shallow mikolov been share, all located with between create, 0.0565226754734803
data, does above earlier version, 0.06184359977153689
you context so, without dimensions many or above, 0.055978810948230755
typically positioned at trained, discerned dimensions, 0.05792147815626114
with nets media be, applications probabilities corpus common, 0.05420216828240454
below why help been, models, 0.05513462259246409
set just playlists or proximity, individual works, 0.056792189559713005
patterns words likelihood human, or, 0.05871748523361049
you understand researchers, led likevec mind context similar, 0.057363129457184
symbolic, turns using, 0.06089596541570127
together looking below unique, symbolic media close implements, 0.05276180459274538
transitional, data earlier below neural, 0.05785992941665649
looking it individual its, features series used related using, 0.053612382908212027
words for semantic trained, states input nets, 0.0521434937643107
google, does space purpose corresponding, 0.057890702807903295
probabilities, feature is, 0.06362240549068525
because may as, intervention mentioned unique, 0.05885368313554277
likevec, below was word or has, 0.05852268582161069
other on space, possible, 0.05618347373821907
embedding analysis unique similarities, produce team for not, 0.0556872153448239
well input, typically co-occurring on will, 0.05546024723607674
deeplearningj, java is, 0.0533140300463587
that spark graphs mathematically, why positioned intervention applications feature, 0.05223070355249346
have applied semantic linguistic, tutorial all they was, 0.055372120691802355
close produces into above network, beyond, 0.05847835906746685
words large, we, 0.06022531122107059
without likes earlier patterns co-occur, may, 0.06327826704293489
distributed, semantic like individual, 0.05568570225902463
nets numerical wordvec, located, 0.05549038272345159
likes, at playlists probabilities, 0.058136924240254954
likelihood located related, or algorithm, 0.055447999208992946
hundred, be purpose they team understand, 0.059907937589159604
they creates embedding followervec latent, discerned above spark word, 0.053310583696933464
without intervention together simply, possible mentioned you together above, 0.0432798820791658
neural, neural while parsing each corresponding, 0.049196774652886396
to simply a created, be group one linguistic wordvec, 0.05505250406520441
these they, produce numerical was, 0.0521114695592376
code dimensions models, linguistic mind nets, 0.0566123645782852
numerical models set similar tomas, wild networks it related been, 0.05467026650533825
two-layer, input positioned created context text, 0.057976753109134735
compared, google series, 0.056035147788932545
parsing input with context implements, possible, 0.054409582787011565
share verbal discrete are together, understand it, 0.05419887827718556
vectorspace does, which just, 0.05690782068815455
output those models probabilities, well may algorithms another why, 0.05462803431073017
embeddings, distributed not the social, 0.0560964197922945
all while, intervention similarities, 0.0548765781866759
in a, mentioned by and corpus you, 0.058673655548447376
context parsing positioned has, as, 0.0526347539213514
purpose as, assigned co-occur latent network it, 0.05920530240225792
java produce, another turns linguistic, 0.05537871288788095
wordvec used probabilities, those corpus, 0.05504970001160772
input produces works looking contexts, several not, 0.056279379582405094
parsing located, output shallow by which net, 0.05511699087783769
help high-dimensional like output related, unique genevec common, 0.05475331346925018
analysis input, many algorithm distributed nets probabilities, 0.053343747320209446
proximity other turns spark, positioned assigned, 0.05494300003501773
high-dimensional, be as, 0.05496870083660819
into the between using vectors, close typically by analysed, 0.05905299983200729
mathematically other typically bayesian, representations because is, 0.05342863550654316
shallow, deep genes feature, 0.054410709582717806
above deeplearningj, being located network, 0.05674673912489663
takes, genes was wordvec features, 0.06512770481561869
are or create likes, dimensions, 0.062013234316945075
large genes produces created, led feature co-occur takes with, 0.05881697135092318
is similarities into common, just, 0.05950399460671842
proposed common followervec deeplearningj because, representations any, 0.05368910558490753
dimensions detects, symbolic, 0.05856456498920918
are intervention linguistic, mikolov, 0.059354607267837964
neural, genes, 0.058718901127576835
discrete mikolov they researchers, algorithm, 0.05719393281615525
deeplearningj, understand located, 0.059395540212996305
representations, for many, 0.05253982375377417
series be, typically, 0.05832859994316101
gpus may without, above, 0.057158956923525746
intervention mikolov human patterns with, distributed numerical word, 0.05441164553848109
works bayesian social discerned another, wild vectorspace net, 0.05200512569160211
above graphs because, at without embedding form, 0.06023308898508028
they between mentioned code verbal, so, 0.05573534961229936
positioned share vectors, likelihood, 0.053691680775481365
intervention have genes two-layer, well understand beyond applications close, 0.053578756365213545
common which, word to share media gpus, 0.05143806186854839
by analysed proximity usefulness, version models social at, 0.056637103097056975
mikolov to the trained words, semantic, 0.060453645657241345
create wordvec, distributed, 0.060812241328097885
analysed proximity, code of, 0.056790186235003176
sentences explained, implements, 0.05473490674830973
have media, deeplearningj, 0.05782198290872947
by co-occurring mind help scala, may, 0.06167331600132585
between processes all, discrete, 0.06662830107859422
tomas, other applied looking can several, 0.059964549052484335
positioned vectorspace wordvec the, typically understand intervention, 0.06002771577256946
corpus, intervention group numerical that, 0.05685265606684517
distributed wild, net, 0.05572517644798383
embedding has why bayesian feature, we latent to vector, 0.05521393752259612
being wild shallow words large, produce social while so, 0.05489796497970074
and models mind implements produce, team algorithm corresponding intervention networks, 0.053752729728490115
explained located numerical created compared, close used has you version, 0.05250755608949811
proposed algorithms is and words, verbal below by, 0.054046863140406694
two-layer above without being, wordvec, 0.06392278216552734
advantages purpose series, representations how intervention understand, 0.05619090914588521
co-occurring, likelihood not algorithms earlier, 0.05691063249087334
networks gpus bayesian human creates, neural many java, 0.05429768786635774
why, series discerned numerical extend, 0.06045785296644084
bayesian, tutorial located advantages in, 0.05921731022205576
individual similar, it such deep together, 0.05540716681125295
playlists, shallow many they transitional, 0.05389961238984205
mentioned we models data, dimensions turns reconstruct, 0.05892900146356907
compared representations be analysis understand, into two-layer, 0.05620837221565246
data features likes code mind, semantic another together of, 0.054803009236942604
similarities explained, common not tutorial takes algorithm, 0.05546577713917941
purpose being, used transitional, 0.05768276118780859
explained, individual algorithms on of creates, 0.05960408729431332
below, led create purpose, 0.06698609333093282
may any is so, on neural, 0.06056535666801408
subsequently, two-layer assigned features likelihood works, 0.055827774171072986
applied extend takes algorithms, embedding applied each, 0.04434981290669909
spark those has usefulness assigned, you does tomas because, 0.05683178222277202
parsing series wild, takes possible not, 0.05442530063170814
many close, two-layer we, 0.054652723373696206
subsequently well reconstruct, scala will numerical, 0.0550878255385321
code we neural, have without these neural, 0.044119062480767635
text, analysis, 0.05557774007320404
each, likelihood media, 0.054155216111451396
input, semantic discerned, 0.055517394809395076
likelihood, verbal data can for, 0.05493527548754309
genevec, share proximity, 0.05407938352567703
is to version reconstruct, are contexts, 0.0570960691431351
gpus beyond it two-layer, many hundred code social, 0.05028704090601764
scala has human vector produce, contexts team turns likes probabilities, 0.05363685866174847
used has mathematically team without, followervec shallow network used scala, 0.0440482827572152
turns co-occur, contexts mikolov to analysis, 0.06050306086363271
states net, mentioned code, 0.05254621042184532
earlier dimensions, corresponding looking feature, 0.056741172712632966
algorithm advantages earlier works analysis, series form, 0.05685320610990375
embeddings in, for code looking net, 0.06143062338104657
space, group turns, 0.060412511861771344
is related vector google, has so patterns proposed, 0.05428808837829158
looking create spark, other words does representations processes, 0.054841627125588314
was detects, nets common, 0.05416973167517036
possible above, human well analysed created, 0.060758584501966834